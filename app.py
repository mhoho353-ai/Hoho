# app.py
"""
Data Analysis Hub - Full-featured Streamlit app
Features:
- Upload CSV / Excel
- Preview + keep original copy (df_original)
- Cleaning toolbox with explanations + auto-suggest cleaning
- EDA: descriptive, diagnostic, correlation, PCA, feature selection, hypothesis testing
- Train/Test split (preserve original)
- Visualization (Plotly + Seaborn) with quick interpretation hints
- Floating Code Editor (write & exec Python on df_working)
- AutoML (tries multiple models for classification/regression/regression)
- Undo/history & local checkpoints
- Export final code & trained model (pickle)
- Designed for good UX: step-by-step but fully flexible
"""

import streamlit as st
st.set_page_config(page_title="Data Analysis Hub", layout="wide", initial_sidebar_state="expanded")

# ---- Imports ----
import pandas as pd
import numpy as np
import io, os, json, pickle, textwrap
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, f_regression
from scipy.stats import ttest_ind
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")

# ---- Constants ----
APP_STATE_DIR = "./.dah_state"
os.makedirs(APP_STATE_DIR, exist_ok=True)

REQUIRED_MODELS = {
    "classification": {
        "LogisticRegression": LogisticRegression,
        "RandomForest": RandomForestClassifier,
        "GradientBoosting": GradientBoostingClassifier,
        "DecisionTree": DecisionTreeClassifier,
        "KNeighbors": KNeighborsClassifier
    },
    "regression": {
        "LinearRegression": LinearRegression,
        "RandomForest": RandomForestRegressor,
        "GradientBoosting": GradientBoostingRegressor,
        "DecisionTree": DecisionTreeRegressor,
        "KNeighbors": KNeighborsRegressor
    },
    "clustering": {
        "KMeans": KMeans
    }
}

CLEANING_FUNCTIONS = {
    "dropna()": "Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø£Ùˆ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø© (NaN).",
    "fillna()": "Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ù‚ÙŠÙ…Ø© Ù…Ø¹ÙŠÙ†Ø© Ù…Ø«Ù„ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø£Ùˆ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø«Ø§Ø¨ØªØ©.",
    "drop_duplicates()": "Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.",
    "astype()": "ØªØ­ÙˆÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¹Ù…ÙˆØ¯ Ù…Ø¹ÙŠÙ† (Ù…Ø«Ù„ Ù†Øµ â†’ Ø±Ù‚Ù…).",
    "replace()": "Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‚ÙŠÙ… Ù…Ø¹ÙŠÙ†Ø© Ø¨Ù‚ÙŠÙ… Ø£Ø®Ø±Ù‰.",
    "str.strip()": "Ø­Ø°Ù Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ.",
    "str.lower()": "ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø©.",
    "str.upper()": "ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ ÙƒØ¨ÙŠØ±Ø©.",
    "apply()": "ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ù…Ø®ØµØµØ© Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ù„ØªÙ†Ø¸ÙŠÙ Ø£Ùˆ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ….",
    "rename()": "Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©.",
    "split()/join()": "ØªÙ‚Ø³ÙŠÙ… Ù†ØµÙˆØµ ÙˆØ¯Ù…Ø¬Ù‡Ø§ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©.",
    "filter()": "ØªØµÙÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø£Ùˆ Ø§Ù„ØµÙÙˆÙ ÙˆÙÙ‚ Ø´Ø±Ø·.",
    "isnull()/notnull()": "ÙƒØ´Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø£Ùˆ ØºÙŠØ± Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©."
}

# ---- Session state init ----
if 'df_original' not in st.session_state:
    st.session_state.df_original = None
if 'df_working' not in st.session_state:
    st.session_state.df_working = None
if 'history' not in st.session_state:
    st.session_state.history = []  # list of dicts {time, action, df_snapshot_csv}
if 'checkpoints' not in st.session_state:
    st.session_state.checkpoints = []
if 'split' not in st.session_state:
    st.session_state.split = {}
if 'pipeline_log' not in st.session_state:
    st.session_state.pipeline_log = []  # user actions and code blocks
if 'models_trained' not in st.session_state:
    st.session_state.models_trained = {}
if 'last_exec_output' not in st.session_state:
    st.session_state.last_exec_output = None

# ---- Helpers ----
def save_checkpoint(state, name_prefix="cp"):
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    filename = f"{name_prefix}_{ts}.json"
    path = os.path.join(APP_STATE_DIR, filename)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(state, f, default=str)
        st.session_state.checkpoints.insert(0, filename)
        return filename
    except Exception as e:
        st.warning(f"Ø®Ø·Ø£ Ø¹Ù†Ø¯ Ø­ÙØ¸ Ø§Ù„Ù†Ø³Ø®Ø©: {e}")
        return None

def snapshot_df(df):
    return df.to_csv(index=False)

def push_history(action):
    entry = {
        "time": str(datetime.utcnow()),
        "action": action,
        "df_csv": snapshot_df(st.session_state.df_working) if st.session_state.df_working is not None else None
    }
    st.session_state.history.append(entry)
    # keep limited history
    if len(st.session_state.history) > 50:
        st.session_state.history.pop(0)

def restore_from_history(index=-1):
    if not st.session_state.history:
        st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØ§Ø±ÙŠØ® Ù„Ù„ØªØ±Ø§Ø¬Ø¹.")
        return
    entry = st.session_state.history[index]
    if entry.get("df_csv"):
        st.session_state.df_working = pd.read_csv(io.StringIO(entry["df_csv"]))
        st.success(f"ØªÙ… Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø­Ø§Ù„Ø©: {entry['action']} Ø¹Ù†Ø¯ {entry['time']}")
    else:
        st.warning("Ù„Ø§ ØªØ­ØªÙˆÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù‚Ø·Ø© Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©.")

def safe_exec(user_code, globals_map):
    """
    Execute user code in provided globals_map context.
    Returns output or exception string.
    """
    try:
        # prepare local namespace
        loc = {}
        exec(user_code, globals_map, loc)
        # capture common variables like df_working, result, plt etc.
        output = {}
        # collect df_working if modified
        if "df_working" in globals_map:
            output["df_working"] = globals_map["df_working"]
        output["locals"] = {k: v for k, v in loc.items() if k not in ("__builtins__",)}
        st.session_state.last_exec_output = output
        return {"ok": True, "output": output}
    except Exception as e:
        return {"ok": False, "error": str(e)}

# ---- Layout ----
# Top header and main description
st.markdown("<h1 style='text-align:center; color:#4A90E2;'>Ø­Ù„Ù‘Ù„ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¨Ø³Ø±Ø¹Ø© â€” Data Analysis Hub</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align:center;'>ÙˆØ§Ø¬Ù‡Ø© Ù…Ø±Ù†Ø© Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©ØŒ Ø£Ùˆ Ù†ÙÙ‘Ø° Ø£ÙŠ ÙƒÙˆØ¯ ÙÙŠ Ø£ÙŠ Ù…Ø±Ø­Ù„Ø© â€” Ø¯ÙˆÙ† Ø§Ù„Ù…Ø³Ø§Ø³ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©.</p>", unsafe_allow_html=True)
st.write("---")

# ---- Sidebar: Upload & Main Controls ----
with st.sidebar:
    st.header("1. Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª & Ø§Ù„Ù†Ø³Ø®")
    uploaded_file = st.file_uploader("Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV Ø£Ùˆ Excel", type=["csv", "xlsx"])
    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith(".csv"):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            st.session_state.df_original = df.copy()
            st.session_state.df_working = df.copy()
            push_history("Uploaded data")
            cp = {"meta": "upload", "time": str(datetime.utcnow())}
            cp["df_csv"] = snapshot_df(df)
            save_checkpoint(cp, name_prefix="upload")
            st.success("ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù ÙˆØ­ÙØ¸ Ù†Ø³Ø®Ø© Ø£ØµÙ„ÙŠØ©.")
            st.write(f"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {df.shape[0]} ØµÙ Ã— {df.shape[1]} Ø¹Ù…ÙˆØ¯")
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")

    if st.button("ğŸ” ØªØ±Ø§Ø¬Ø¹ (Undo Ø¢Ø®Ø± Ø®Ø·ÙˆØ©)"):
        if st.session_state.history:
            # pop last (which is current state) and restore previous
            if len(st.session_state.history) >= 2:
                st.session_state.history.pop()  # remove current
                restore_from_history(-1)
            else:
                st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø®Ø·ÙˆØ© Ø£Ù‚Ø¯Ù… Ù„Ù„ØªØ±Ø§Ø¬Ø¹ Ø¥Ù„ÙŠÙ‡Ø§.")
        else:
            st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØ§Ø±ÙŠØ® Ù„Ù„ØªØ±Ø§Ø¬Ø¹.")

    st.markdown("---")
    st.header("2. Ù†Ù‚Ø§Ø· Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©")
    if st.session_state.checkpoints:
        sel_cp = st.selectbox("Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù…Ù† Ù†Ø³Ø®Ø© Ù…Ø­ÙÙˆØ¸Ø©:", options=st.session_state.checkpoints)
        if st.button("Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù†Ø³Ø®Ø©"):
            try:
                with open(os.path.join(APP_STATE_DIR, sel_cp), "r", encoding="utf-8") as f:
                    cp = json.load(f)
                if cp.get("df_csv"):
                    st.session_state.df_working = pd.read_csv(io.StringIO(cp["df_csv"]))
                    push_history(f"restore:{sel_cp}")
                    st.success("ØªÙ… Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©.")
            except Exception as e:
                st.error(f"ÙØ´Ù„ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù†Ø³Ø®Ø©: {e}")

    st.markdown("---")
    st.header("3. Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø³Ø±ÙŠØ¹Ø©")
    if st.button("Ø­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¢Ù†"):
        cp = {"meta": "manual_save", "time": str(datetime.utcnow())}
        cp["df_csv"] = snapshot_df(st.session_state.df_working) if st.session_state.df_working is not None else ""
        fn = save_checkpoint(cp, name_prefix="manual")
        if fn:
            st.success(f"ØªÙ… Ø­ÙØ¸ Ù†Ø³Ø®Ø©: {fn}")

    if st.button("ØªÙØ±ÙŠØº ÙƒÙ„ Ø§Ù„Ø­Ø§Ù„Ø§Øª (Reset app state)"):
        st.session_state.df_original = None
        st.session_state.df_working = None
        st.session_state.history = []
        st.session_state.checkpoints = []
        st.session_state.pipeline_log = []
        st.success("ØªÙ… ØªÙØ±ÙŠØº Ø§Ù„Ø­Ø§Ù„Ø©.")

# ---- Main Tabs: EDA / Cleaning / Analysis / Split / Visualize / AutoML / Code Editor / Export
tabs = st.tabs(["Ø§Ø³ØªÙƒØ´Ø§Ù (Preview)", "ØªÙ†Ø¸ÙŠÙ (Cleaning)", "ØªØ­Ù„ÙŠÙ„ (Analysis)", "ØªÙ‚Ø³ÙŠÙ… (Split)", "ØªØµÙˆÙŠØ± (Visualize)", "AutoML", "ÙˆØ­Ø¯Ø© Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ (Code)", "ØªØµØ¯ÙŠØ± / ØªÙ†Ø²ÙŠÙ„"])

# ---------------- Tab 1: Preview ----------------
with tabs[0]:
    st.header("Ø§Ø³ØªÙƒØ´Ø§Ù Ø¹Ø§Ù… â€” Preview")
    if st.session_state.df_working is None:
        st.info("Ø§Ø±ÙØ¹ Ù…Ù„ÙÙ‹Ø§ Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù.")
    else:
        df = st.session_state.df_working
        st.subheader("Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© (Ø£ÙˆÙ„ 10 ØµÙÙˆÙ)")
        st.dataframe(df.head(10))

        st.subheader("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø³Ø±ÙŠØ¹Ø©")
        c1, c2, c3 = st.columns(3)
        c1.metric("ØµÙÙˆÙ", df.shape[0])
        c2.metric("Ø£Ø¹Ù…Ø¯Ø©", df.shape[1])
        c3.metric("Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø§Ù„ÙƒÙ„ÙŠØ©", int(df.isnull().sum().sum()))

        if st.checkbox("Ø¹Ø±Ø¶ ÙˆØµÙ ÙƒØ§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª (describe + dtypes)"):
            st.write(df.describe(include='all'))
            st.write(df.dtypes)

        if st.button("Ø§Ù‚ØªØ±Ø§Ø­ ØªÙ†Ø¸ÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠ"):
            # simple suggestions
            suggestions = []
            if df.isnull().sum().sum() > 0:
                suggestions.append("ÙŠÙˆØ¬Ø¯ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø© â€” Ø§Ù‚ØªØ±Ø­ fillna() Ø£Ùˆ dropna() Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…ÙˆØ¯.")
            if df.duplicated().sum() > 0:
                suggestions.append("ÙŠÙˆØ¬Ø¯ ØµÙÙˆÙ Ù…ÙƒØ±Ø±Ø© â€” Ø§Ù‚ØªØ±Ø­ drop_duplicates().")
            text_cols = df.select_dtypes(include="object").columns.tolist()
            if text_cols:
                suggestions.append(f"Ø£Ø¹Ù…Ø¯Ø© Ù†ØµÙŠØ© Ù‚Ø¯ ØªØ­ØªØ§Ø¬ strip() Ø£Ùˆ lower(): {text_cols[:5]}")
            st.write("Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª:")
            for s in suggestions:
                st.write("- ", s)

# ---------------- Tab 2: Cleaning ----------------
with tabs[1]:
    st.header("ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    if st.session_state.df_working is None:
        st.info("ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ù…Ù„Ù Ø£ÙˆÙ„Ù‹Ø§.")
    else:
        df = st.session_state.df_working

        st.write("Ù…Ø±Ù‘Ø± Ø¹Ù„Ù‰ Ø£ÙŠ Ø¯Ø§Ù„Ø© Ù„Ù‚Ø±Ø§Ø¡Ø© Ø´Ø±Ø­Ù‡Ø§.")
        func_choice = st.selectbox("Ø§Ø®ØªØ± Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ:", options=list(CLEANING_FUNCTIONS.keys()), format_func=lambda x: x)
        st.caption(CLEANING_FUNCTIONS[func_choice])

        st.markdown("**Ø®ÙŠØ§Ø±Ø§Øª Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªÙ†Ø¸ÙŠÙ:**")
        col1, col2, col3 = st.columns(3)
        if col1.button("Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø§Ù„ÙˆØ³ÙŠØ· (median)"):
            push_history("fillna_median")
            num_cols = st.session_state.df_working.select_dtypes(include=np.number).columns
            st.session_state.df_working[num_cols] = st.session_state.df_working[num_cols].fillna(st.session_state.df_working[num_cols].median())
            st.success("ØªÙ… Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ© Ø¨Ø§Ù„ÙˆØ³ÙŠØ·.")
        if col2.button("Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ NA"):
            push_history("dropna_rows")
            st.session_state.df_working = st.session_state.df_working.dropna(axis=0)
            st.success("ØªÙ… Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ NA.")
        if col3.button("Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©"):
            push_history("drop_duplicates")
            st.session_state.df_working = st.session_state.df_working.drop_duplicates()
            st.success("ØªÙ… Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©.")

        st.markdown("**ØªÙ†Ø¸ÙŠÙ Ù†ØµÙˆØµ Ø³Ø±ÙŠØ¹**")
        text_cols = st.session_state.df_working.select_dtypes(include="object").columns.tolist()
        sel_text = st.multiselect("Ø§Ø®ØªØ± Ø£Ø¹Ù…Ø¯Ø© Ù†ØµÙŠØ© Ù„ØªØ·Ø¨ÙŠÙ‚ strip() Ùˆlower():", options=text_cols)
        if st.button("ØªØ·Ø¨ÙŠÙ‚ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ"):
            if sel_text:
                push_history("clean_text")
                for c in sel_text:
                    st.session_state.df_working[c] = st.session_state.df_working[c].astype(str).str.strip().str.lower()
                st.success(f"ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {sel_text}")
            else:
                st.info("Ù„Ù… ØªØ®ØªÙØ± Ø£Ø¹Ù…Ø¯Ø© Ù†ØµÙŠØ©.")

        st.markdown("---")
        st.subheader("ØªØ®ØµÙŠØµ Ø¹Ù…Ù„ÙŠÙ‘Ø© ØªÙ†Ø¸ÙŠÙ (Advanced)")
        st.write("ÙŠÙ…ÙƒÙ†Ùƒ ÙƒØªØ§Ø¨Ø© Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ Ù…Ø®ØµØµØ© ÙÙŠ Ù…Ø­Ø±Ù‘Ø± Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ ÙˆØªØ´ØºÙŠÙ„Ù‡Ø§ Ø¹Ù„Ù‰ df_working.")

# ---------------- Tab 3: Analysis ----------------
with tabs[2]:
    st.header("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    if st.session_state.df_working is None:
        st.info("Ø§Ø±ÙØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„.")
    else:
        df = st.session_state.df_working
        st.write("Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø«Ù… Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©.")
        analysis_type = st.selectbox("Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„:", [
            "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØµÙÙŠ - Descriptive",
            "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø®ÙŠØµÙŠ - Diagnostic",
            "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª - Correlation",
            "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ (PCA) - Factor",
            "Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª - Feature Selection",
            "Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª - Hypothesis Testing"
        ])
        cols = st.multiselect("Ø§Ø®ØªØ± Ø£Ø¹Ù…Ø¯Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„:", options=df.columns.tolist())
        if not cols:
            st.info("Ø§Ø®ØªØ± Ø¹Ù…ÙˆØ¯Ù‹Ø§ ÙˆØ§Ø­Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ù„Ù…Ø¶ÙŠ Ù‚Ø¯Ù…Ù‹Ø§.")
        else:
            if "Descriptive" in analysis_type:
                st.subheader("Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØµÙÙŠ")
                st.dataframe(df[cols].describe(include='all'))
            if "Diagnostic" in analysis_type:
                st.subheader("Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø®ÙŠØµÙŠ")
                st.write("Missing per column:")
                st.write(df[cols].isnull().sum())
                st.write("Basic distributions:")
                st.write(df[cols].describe(include='all'))
            if "Correlation" in analysis_type:
                st.subheader("Correlation Matrix")
                num_df = df[cols].select_dtypes(include=np.number)
                corr = num_df.corr()
                st.dataframe(corr)
                fig = px.imshow(corr, text_auto=True)
                st.plotly_chart(fig, use_container_width=True)
                # quick interpretation
                st.write("ØªÙØ³ÙŠØ± Ø³Ø±ÙŠØ¹:")
                high_corr = []
                for i in corr.columns:
                    for j in corr.columns:
                        if i!=j and abs(corr.loc[i,j])>0.7:
                            high_corr.append((i,j,corr.loc[i,j]))
                if high_corr:
                    st.success(f"ÙˆØ¬Ø¯Øª Ø¹Ù„Ø§Ù‚Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø¨ÙŠÙ†: {high_corr[:5]}")
                else:
                    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù„Ø§Ù‚Ø§Øª Ù‚ÙˆÙŠØ© (>0.7) Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©.")
            if "PCA" in analysis_type:
                st.subheader("PCA - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„")
                num_df = df[cols].select_dtypes(include=np.number).dropna()
                if num_df.shape[1] < 2:
                    st.error("Ø£Ø­ØªØ§Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ø¹Ù…ÙˆØ¯ÙŠÙ† Ø±Ù‚Ù…ÙŠÙŠÙ† Ù„Ù€ PCA.")
                else:
                    pca = PCA(n_components=min(3, num_df.shape[1]))
                    comps = pca.fit_transform(num_df)
                    comp_df = pd.DataFrame(comps, columns=[f"PC{i+1}" for i in range(comps.shape[1])])
                    st.write("Explained variance ratio:", pca.explained_variance_ratio_)
                    fig = px.scatter(comp_df, x="PC1", y="PC2")
                    st.plotly_chart(fig, use_container_width=True)
            if "Feature Selection" in analysis_type:
                st.subheader("Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª")
                target_col = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù (Target):", options=df.columns.tolist(), index=0)
                k = st.slider("ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª ØªØ±ÙŠØ¯ Ø§Ø®ØªÙŠØ§Ø±Ù‡Ø§ØŸ", 1, max(1, len(df.columns)-1), 3)
                if st.button("ØªØ´ØºÙŠÙ„ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª"):
                    try:
                        X = df.drop(columns=[target_col]).select_dtypes(include=np.number).dropna()
                        y = df[target_col]
                        selector = SelectKBest(score_func=(f_classif if y.dtype.kind in 'biufc' else f_classif), k=k)
                        selector.fit(X, y)
                        scores = pd.Series(selector.scores_, index=X.columns).sort_values(ascending=False)
                        st.write(scores.head(k))
                    except Exception as e:
                        st.error(f"ÙØ´Ù„ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª: {e}")
            if "Hypothesis Testing" in analysis_type:
                st.subheader("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª (T-test)")
                c1 = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ÙˆÙ„:", options=df.columns.tolist(), index=0)
                c2 = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø«Ø§Ù†ÙŠ:", options=df.columns.tolist(), index=0)
                if st.button("ØªØ´ØºÙŠÙ„ T-test"):
                    try:
                        stat, p = ttest_ind(df[c1].dropna(), df[c2].dropna())
                        st.write(f"T-stat: {stat:.4f}  P-value: {p:.6f}")
                        if p < 0.05:
                            st.success("ÙŠÙˆØ¬Ø¯ ÙØ±Ù‚ Ø°Ùˆ Ø¯Ù„Ø§Ù„Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ© (p < 0.05).")
                        else:
                            st.info("Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙØ±Ù‚ Ø°Ùˆ Ø¯Ù„Ø§Ù„Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ©.")
                    except Exception as e:
                        st.error(f"ÙØ´Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}")

# ---------------- Tab 4: Split ----------------
with tabs[3]:
    st.header("ğŸ”€ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Train / Test)")
    if st.session_state.df_working is None:
        st.info("Ø£Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„Ø§.")
    else:
        df = st.session_state.df_working
        st.write("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù (Target) ÙˆØ§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù…ÙŠØ²Ø© (Features).")
        target = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù:", options=df.columns.tolist())
        features = st.multiselect("Ø§Ø®ØªØ± Ø§Ù„Ù…Ø²Ø§ÙŠØ§ (Ø§ØªØ±Ùƒ ÙØ§Ø±ØºØ§ Ù„Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ©):", options=[c for c in df.columns.tolist() if c!=target])
        test_size = st.slider("Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (test_size):", 0.05, 0.5, 0.2)
        stratify_option = None
        if st.button("Ù†ÙÙ‘Ø° Ø§Ù„ØªÙ‚Ø³ÙŠÙ…"):
            # prepare X,y
            if not features:
                X = df.drop(columns=[target]).select_dtypes(include=np.number)
            else:
                X = df[features]
            y = df[target]
            try:
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=(y if y.nunique()<50 else None))
            except Exception:
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
            st.session_state.split = {
                "X_train": X_train, "X_test": X_test, "y_train": y_train, "y_test": y_test,
                "features": X.columns.tolist(), "target": target
            }
            push_history("split_data")
            st.success(f"ØªÙ… Ø§Ù„ØªÙ‚Ø³ÙŠÙ…: {len(X_train)} ØªØ¯Ø±ÙŠØ¨ / {len(X_test)} Ø§Ø®ØªØ¨Ø§Ø±")
            st.write("Ù…Ù…ÙŠØ²Ø§Øª:", st.session_state.split["features"])

# ---------------- Tab 5: Visualize ----------------
with tabs[4]:
    st.header("ğŸ“ˆ ØªØµÙˆÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    if st.session_state.df_working is None:
        st.info("Ø£Ø±ÙØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØµÙˆÙŠØ±Ù‡Ø§.")
    else:
        df = st.session_state.df_working
        viz_type = st.selectbox("Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø±Ø³Ù…:", ["Histogram", "Scatter", "Line", "Bar", "Box", "Pairplot", "Correlation Heatmap"])
        if viz_type in ["Histogram", "Box", "Bar", "Line", "Scatter"]:
            col_x = st.selectbox("Ø§Ù„Ù…Ø­ÙˆØ± X / Ø§Ù„Ø¹Ù…ÙˆØ¯:", options=df.columns.tolist())
            if viz_type == "Scatter":
                col_y = st.selectbox("Ø§Ù„Ù…Ø­ÙˆØ± Y:", options=[c for c in df.columns if c!=col_x])
            else:
                col_y = None
            if st.button("Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ù…"):
                try:
                    if viz_type == "Histogram":
                        fig = px.histogram(df, x=col_x, marginal="box")
                    elif viz_type == "Box":
                        fig = px.box(df, y=col_x)
                    elif viz_type == "Bar":
                        ct = df[col_x].value_counts().reset_index()
                        ct.columns = [col_x, "count"]
                        fig = px.bar(ct, x=col_x, y="count")
                    elif viz_type == "Line":
                        fig = px.line(df, x=df.index, y=col_x)
                    elif viz_type == "Scatter":
                        fig = px.scatter(df, x=col_x, y=col_y)
                    st.plotly_chart(fig, use_container_width=True)
                    # quick interpretation
                    st.write("ØªÙØ³ÙŠØ± Ù…Ø¨Ø³Ù‘Ø·:")
                    if viz_type == "Histogram":
                        st.write("ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø£Ùˆ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© (outliers) ÙÙŠ Ø§Ù„ØªÙˆØ²ÙŠØ¹.")
                    if viz_type == "Scatter":
                        st.write("Ø§Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±ÙŠÙ† Ø¥Ù† ÙˆÙØ¬Ø¯Øª (Ø®Ø·ÙŠØ©/ØºÙŠØ± Ø®Ø·ÙŠØ©).")
                except Exception as e:
                    st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø±Ø³Ù…: {e}")
        elif viz_type == "Pairplot":
            sel = st.multiselect("Ø§Ø®ØªØ± Ø£Ø¹Ù…Ø¯Ø©:", options=df.select_dtypes(include=np.number).columns.tolist(), default=df.select_dtypes(include=np.number).columns.tolist()[:4])
            if st.button("Ø¹Ø±Ø¶ Pairplot"):
                try:
                    fig = sns.pairplot(df[sel].dropna().sample(min(500, len(df))))
                    st.pyplot(fig)
                except Exception as e:
                    st.error(f"ÙØ´Ù„: {e}")
        else:  # Correlation Heatmap
            num = df.select_dtypes(include=np.number)
            if num.shape[1] < 2:
                st.info("ØªØ­ØªØ§Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ø¹Ù…ÙˆØ¯ÙŠÙ† Ø¹Ø¯Ø¯ÙŠÙŠÙ† Ù„Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·ÙŠØ©.")
            else:
                corr = num.corr()
                fig = px.imshow(corr, text_auto=True)
                st.plotly_chart(fig, use_container_width=True)
                st.write("ØªÙØ³ÙŠØ±: Ù‚ÙŠÙ… Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† 1 Ø£Ùˆ -1 ØªØ¯Ù„ Ø¹Ù„Ù‰ Ø§Ø±ØªØ¨Ø§Ø· Ù‚ÙˆÙŠ.")

# ---------------- Tab 6: AutoML ----------------
with tabs[5]:
    st.header("ğŸ¤– AutoML â€” ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆÙ‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡")
    if st.session_state.df_working is None:
        st.info("Ø£Ø±ÙØ¹ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØºÙŠÙ‘Ø±Ù‡Ø§ Ø«Ù… Ø¹Ø¯ Ù‡Ù†Ø§.")
    else:
        df = st.session_state.df_working
        task = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©:", ["Classification", "Regression", "Clustering"])
        if task in ["Classification", "Regression"]:
            # choose target
            target = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù:", options=df.columns.tolist())
            features = st.multiselect("Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Features (Ø§ØªØ±Ùƒ ÙØ§Ø±ØºÙ‹Ø§ Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©):", options=[c for c in df.columns if c!=target])
            test_size = st.slider("Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:", 0.05, 0.4, 0.2)
            if st.button("ØªØ´ØºÙŠÙ„ AutoML"):
                # prepare X,y
                if not features:
                    X = df.drop(columns=[target]).select_dtypes(include=np.number)
                else:
                    X = df[features]
                y = df[target]
                # Train/test split
                try:
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=(y if task=="Classification" and y.nunique()<50 else None))
                except Exception:
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
                # simple preprocessor: impute + scale for numeric, encode cats if any
                numeric_cols = X_train.select_dtypes(include=np.number).columns.tolist()
                cat_cols = X_train.select_dtypes(include=["object", "category"]).columns.tolist()
                num_pipe = Pipeline([("impute", SimpleImputer(strategy="median")), ("scale", StandardScaler())])
                cat_pipe = Pipeline([("impute", SimpleImputer(strategy="most_frequent")), ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=False))]) if cat_cols else None
                preprocessor = ColumnTransformer(
                    transformers=[("num", num_pipe, numeric_cols)] + ([("cat", cat_pipe, cat_cols)] if cat_cols else []),
                    remainder="drop"
                )
                models_to_try = REQUIRED_MODELS["classification" if task=="Classification" else "regression"]
                results = []
                for name, cls in models_to_try.items():
                    try:
                        pipe = Pipeline([("pre", preprocessor), ("model", cls())])
                        pipe.fit(X_train, y_train)
                        preds = pipe.predict(X_test)
                        if task == "Classification":
                            acc = accuracy_score(y_test, preds)
                            f1 = f1_score(y_test, preds, average="weighted") if len(np.unique(y_test))>1 else f1_score(y_test, preds, average="macro")
                            results.append({"model": name, "acc": acc, "f1": f1, "estimator": pipe})
                        else:
                            mse = mean_squared_error(y_test, preds)
                            r2 = r2_score(y_test, preds)
                            results.append({"model": name, "mse": mse, "r2": r2, "estimator": pipe})
                    except Exception as e:
                        st.write(f"ÙØ´Ù„ Ù†Ù…ÙˆØ°Ø¬ {name}: {e}")
                # show results
                if results:
                    st.write("Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:")
                    if task == "Classification":
                        res_df = pd.DataFrame(results)[["model","acc","f1"]].sort_values("acc", ascending=False)
                        st.dataframe(res_df)
                        best = max(results, key=lambda r: r["acc"])
                    else:
                        res_df = pd.DataFrame(results)[["model","r2","mse"]].sort_values("r2", ascending=False)
                        st.dataframe(res_df)
                        best = max(results, key=lambda r: r["r2"])
                    st.success(f"Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best['model']}")
                    # save best
                    st.session_state.models_trained["best"] = best
                    # option to download model
                    if st.button("Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„ (pickle)"):
                        fn = f"best_model_{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}.pkl"
                        with open(fn, "wb") as f:
                            pickle.dump(best["estimator"], f)
                        st.success(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ {fn}")
                        with open(fn, "rb") as f:
                            st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (pickle)", data=f, file_name=fn)
                else:
                    st.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ØµØ§Ù„Ø­Ø©.")

        else:  # Clustering
            n_clusters = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª (n_clusters):", 2, 20, 5)
            numeric = df.select_dtypes(include=np.number).dropna()
            if numeric.shape[1] < 1:
                st.info("ØªØ­ØªØ§Ø¬ Ø£Ø¹Ù…Ø¯Ø© Ø±Ù‚Ù…ÙŠØ© Ù„Ù„Ù€ Clustering.")
            else:
                if st.button("ØªØ´ØºÙŠÙ„ KMeans"):
                    k = KMeans(n_clusters=n_clusters, random_state=42)
                    labels = k.fit_predict(numeric)
                    st.session_state.df_working["_cluster"] = labels
                    st.success("ØªÙ… ØªÙ†ÙÙŠØ° KMeans ÙˆØ¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ _cluster")
                    st.write(pd.Series(labels).value_counts())

# ---------------- Tab 7: Code Editor (Floating-like) ----------------
with tabs[6]:
    st.header("ğŸ§¾ Ù…Ø­Ø±Ø± Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ â€” Ù†ÙÙ‘Ø° Ø£ÙŠ ÙƒÙˆØ¯ ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù†")
    st.write("ÙŠÙ…ÙƒÙ†Ùƒ ÙƒØªØ§Ø¨Ø© ÙƒÙˆØ¯ Python Ù‡Ù†Ø§ ÙˆØªØ´ØºÙŠÙ„Ù‡ Ø¹Ù„Ù‰ `df_working` Ø£Ùˆ Ø£ÙŠ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù€ pipeline.")
    st.info("Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„ØªÙ†ÙÙŠØ° Ø³ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ù†Ø³Ø®Ø© Ø§Ù„Ø¹Ù…Ù„ df_working ÙÙ‚Ø· (Ù…Ø§ Ù„Ù… ØªÙØ¹Ø¯Ù‘Ù„ df_original ÙŠØ¯ÙˆÙŠÙ‹Ø§).")

    # Execution target
    exec_target = st.selectbox("Ù†ÙÙ‘Ø° Ø§Ù„ÙƒÙˆØ¯ Ø¹Ù„Ù‰:", options=[
        "df_working (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø¹Ø§Ù…Ù„Ø©)",
        "Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ (pre)",
        "Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ (post_clean)",
        "Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø³ÙŠÙ… (post_split)",
        "Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (models)",
        "ØªØ­Ù…ÙŠÙ„/ØªØµØ¯ÙŠØ± (export)"
    ])

    code_area = st.text_area("Ø§ÙƒØªØ¨ ÙƒÙˆØ¯ Python Ù‡Ù†Ø§:", value=textwrap.dedent("""# Ù…Ø«Ø§Ù„:
# df_working['new_col'] = df_working['some_numeric_col'] * 2
# def mark_outliers(df):
#     df['is_outlier'] = (df['new_col'] > df['new_col'].quantile(0.99))
#     return df
# df_working = mark_outliers(df_working)
pass
"""), height=250)

    run_col, save_col = st.columns([1,1])
    if run_col.button("ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¢Ù†"):
        # prepare safe globals
        globals_map = {
            "pd": pd, "np": np, "plt": plt, "sns": sns, "px": px,
            "df_original": st.session_state.df_original,
            "df_working": st.session_state.df_working,
            "X_train": st.session_state.split.get("X_train"),
            "X_test": st.session_state.split.get("X_test"),
            "y_train": st.session_state.split.get("y_train"),
            "y_test": st.session_state.split.get("y_test"),
            # sklearn available
            "RandomForestClassifier": RandomForestClassifier,
            "RandomForestRegressor": RandomForestRegressor,
            "LinearRegression": LinearRegression,
            "LogisticRegression": LogisticRegression
        }
        res = safe_exec(code_area, globals_map)
        if res["ok"]:
            # update df_working if changed
            if "df_working" in globals_map and globals_map["df_working"] is not None:
                st.session_state.df_working = globals_map["df_working"]
                push_history("exec_code")
                st.success("ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯ ÙˆØªØ­Ø¯ÙŠØ« df_working.")
            st.write("Ù†Ø§ØªØ¬ Ø§Ù„ØªÙ†ÙÙŠØ° (Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯):")
            st.write(res["output"].get("locals", {}))
        else:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†ÙÙŠØ°: {res['error']}")

    if save_col.button("Ø­ÙØ¸ ÙƒØªÙ„Ø© ÙƒØ¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø¨Ø§ÙŠØ¨Ù„Ø§ÙŠÙ†"):
        block = {
            "time": str(datetime.utcnow()),
            "target": exec_target,
            "code": code_area
        }
        st.session_state.pipeline_log.append(block)
        push_history("save_code_block")
        st.success("ØªÙ… Ø­ÙØ¸ ÙƒØªÙ„Ø© Ø§Ù„ÙƒÙˆØ¯ ÙÙŠ Ø³Ø¬Ù„ Ø§Ù„Ø¨Ø§ÙŠØ¨Ù„Ø§ÙŠÙ†.")

    if st.session_state.pipeline_log:
        st.markdown("**Ø³Ø¬Ù„ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ù…ÙØ­ÙÙˆØ¸Ø©:**")
        for i, b in enumerate(st.session_state.pipeline_log[::-1]):
            st.markdown(f"- [{b['time']}] target={b['target']} â€” code preview: `{b['code'][:80].replace('\\n',' ')}...`")
            if st.button(f"ØªØ´ØºÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„ÙƒØªÙ„Ø© #{len(st.session_state.pipeline_log)-i-1}"):
                # run the block
                globals_map = {
                    "pd": pd, "np": np, "plt": plt, "sns": sns, "px": px,
                    "df_original": st.session_state.df_original,
                    "df_working": st.session_state.df_working,
                    "X_train": st.session_state.split.get("X_train"),
                    "X_test": st.session_state.split.get("X_test"),
                    "y_train": st.session_state.split.get("y_train"),
                    "y_test": st.session_state.split.get("y_test"),
                }
                res = safe_exec(b["code"], globals_map)
                if res["ok"]:
                    if "df_working" in globals_map and globals_map["df_working"] is not None:
                        st.session_state.df_working = globals_map["df_working"]
                        push_history("exec_saved_block")
                        st.success("ØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØªÙ„Ø© ÙˆØªØ­Ø¯ÙŠØ« df_working.")
                else:
                    st.error(f"ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØªÙ„Ø©: {res['error']}")

# ---------------- Tab 8: Export / Download ----------------
with tabs[7]:
    st.header("ğŸ“¦ ØªØµØ¯ÙŠØ± Ùˆ ØªÙ†Ø²ÙŠÙ„")
    st.write("ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠÙƒØ±Ø± Ù…Ø§ Ù‚Ù…Øª Ø¨Ù‡ØŒ Ø£Ùˆ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø£Ùˆ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„.")
    if st.session_state.df_working is None:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ø¹Ø¯.")
    else:
        # Export working data
        buf = io.StringIO()
        st.session_state.df_working.to_csv(buf, index=False)
        st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ù†Ø³Ø®Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø¯Ù„Ø© (CSV)", data=buf.getvalue(), file_name="df_working.csv", mime="text/csv")

        # Export pipeline log as script (generate python script)
        if st.button("Ø¥Ù†Ø´Ø§Ø¡ ÙƒÙˆØ¯ Python Ù…Ù† Ø§Ù„Ø³Ø¬Ù„ (ØªÙƒØ±Ø§Ø± Ø§Ù„Ø®Ø·ÙˆØ§Øª)"):
            # Build simple script
            script_lines = [
                "# Generated pipeline script from Data Analysis Hub",
                "import pandas as pd, numpy as np",
                "from sklearn.model_selection import train_test_split",
                ""
            ]
            script_lines.append("# Load data (user should replace path)")
            script_lines.append("df = pd.read_csv('your_data.csv')\n")
            for step in st.session_state.pipeline_log:
                script_lines.append("# --- Block saved at: " + step["time"])
                script_lines.append(step["code"])
                script_lines.append("\n")
            script_text = "\n".join(script_lines)
            st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ ÙƒÙˆØ¯ pipeline (script.py)", data=script_text, file_name="pipeline_script.py", mime="text/x-python")
            st.code(script_text[:1000] + "\n\n# ... (full script available for download)")

        # Export best model if exists
        if st.session_state.models_trained.get("best"):
            best = st.session_state.models_trained["best"]
            if st.button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ (pickle)"):
                fn = f"best_model_{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}.pkl"
                with open(fn, "wb") as f:
                    pickle.dump(best["estimator"], f)
                with open(fn, "rb") as f:
                    st.download_button("ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (pickle)", data=f, file_name=fn)

        # Export full app code (this file)
        if st.button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙƒØ§Ù…Ù„ (app.py)"):
            try:
                with open(__file__, "r", encoding="utf-8") as f:
                    content = f.read()
            except Exception:
                # fallback: export a helpful message
                content = "# Ø¶Ø¹ Ù‡Ù†Ø§ ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ø§ÙØªØ­ app.py Ù…Ø­Ù„ÙŠÙ‹Ø§ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯."
            st.download_button("ØªØ­Ù…ÙŠÙ„ app.py", data=content, file_name="app.py", mime="text/x-python")

st.markdown("---")
st.caption("Data Analysis Hub â€” ØªÙ… ØªØ¬Ù‡ÙŠØ²Ù‡ Ù„ÙŠÙƒÙˆÙ† Ø³Ù‡Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…ØŒ Ù…Ø±Ù†ØŒ ÙˆÙ‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹Ø©. Ø¥Ø°Ø§ Ø±ØºØ¨Øª ÙÙŠ Ø¥Ø¶Ø§ÙØ© AutoML Ø£Ø¹Ù…Ù‚ (Bayesian tuning, ensembling Ù…ØªÙ‚Ø¯Ù…, AutoCV) Ø£Ùˆ ÙˆØ§Ø¬Ù‡Ø© White-label ÙˆØ³Ø¬Ù„ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ Ø£Ø®Ø¨Ø±Ù†ÙŠ ÙˆØ³Ø£Ù‚ÙˆÙ… Ø¨ÙƒØªØ§Ø¨ØªÙ‡ Ù„Ùƒ ÙƒØ®Ø·ÙˆØ© Ø«Ø§Ù†ÙŠØ©.")
